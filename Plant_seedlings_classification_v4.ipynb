{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of ML_v4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "rLh01_NYPtRv",
        "hK5ZR-1HPtR9",
        "GMMj4BxJPtSB",
        "D9Bc8gZ7PtSN",
        "xAx-ZMqkPtSZ",
        "i2ErvhpF49FH"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TeyrCrimson/tensorflow-projects/blob/master/Plant_seedlings_classification_v4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "wJFgjnqPLD2s",
        "outputId": "7a898725-0c8e-4fdf-afbf-9117aa236555",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0i_4ZpjqDpn7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_dir = '/content/drive/My Drive/Machine Learning/plant-seedlings-classification/train'\n",
        "validation_dir = '/content/drive/My Drive/Machine Learning/plant-seedlings-classification/test/test'\n",
        "image_size = 224"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "_E-YInrLeJXC",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "sample_submission = pd.read_csv('/content/drive/My Drive/Machine Learning/plant-seedlings-classification/sample_submission/sample_submission.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ZhG0BLe4ecpp",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "CATEGORIES = ['Black-grass', 'Charlock', 'Cleavers', 'Common Chickweed', 'Common wheat', 'Fat Hen', 'Loose Silky-bent',\n",
        "              'Maize', 'Scentless Mayweed', 'Shepherds Purse', 'Small-flowered Cranesbill', 'Sugar beet']\n",
        "NUM_CATEGORIES = len(CATEGORIES)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "uPfANXrkLFjr",
        "outputId": "18e7fb9b-171c-4b18-8f88-ddcbe49ee805",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
        "from keras.preprocessing import image\n",
        "from keras.layers import Dropout, Flatten, Dense, LeakyReLU\n",
        "from keras.applications import ResNet50, VGG19\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import GlobalAveragePooling2D, Conv2D, BatchNormalization, MaxPooling2D\n",
        "from keras import backend as K\n",
        "from keras import optimizers\n",
        "from keras.applications.resnet50 import preprocess_input\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "import cv2\n",
        "import glob\n",
        "import datetime"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "rSsX44o0LhWS",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def hsvmask(img):\n",
        "    blur = cv2.GaussianBlur(img, (5,5), 0)\n",
        "    hsv = cv2.cvtColor(blur, cv2.COLOR_RGB2HSV)\n",
        "    lower = np.array([45,0,0])\n",
        "    upper = np.array([95,255,255])\n",
        "    mask = cv2.inRange(hsv,lower,upper)\n",
        "    #struc = cv2.getStructuringElement(cv2.MORPH_RECT,(11,11))\n",
        "    #mask = cv2.morphologyEx(mask,cv2.MORPH_CLOSE,struc)\n",
        "    #boolean = mask>0\n",
        "    #boolean = np.dstack([boolean]*3)\n",
        "    new = np.zeros_like(img,np.uint8)\n",
        "    new = cv2.bitwise_and(img,img,mask=mask)\n",
        "    return new"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WJ4xIB66c_kb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def hsv(img):\n",
        "    blur = cv2.GaussianBlur(img, (5,5), 0)\n",
        "    hsv = cv2.cvtColor(blur, cv2.COLOR_RGB2HSV)\n",
        "    return hsv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gZpDZ-MgMmE_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def remove_white(img):\n",
        "  gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "  ret,thresh = cv2.threshold(gray, 240, 255, cv2.THRESH_BINARY)\n",
        "  img[thresh == 255] = 0\n",
        "  kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5,5))\n",
        "  erosion = cv2.erode(img, kernel, iterations=1)\n",
        "  return erosion"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "qAGG0LnCV_4w",
        "outputId": "ce787122-5032-469a-ecfa-55557737fea8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train = []\n",
        "y_train1=[]\n",
        "test = []\n",
        "train1=[]\n",
        "validation=[]\n",
        "\n",
        "for category_id, category in enumerate(CATEGORIES):\n",
        "    for file in glob.glob(os.path.join('{}/{}/*.png'.format(train_dir,category))):\n",
        "        train.append([file, category_id, category])\n",
        "        image = cv2.imread (file)\n",
        "        image = cv2.resize(remove_white(hsvmask(image)),(image_size,image_size))\n",
        "        #image = load_img(file, target_size=(200, 200))\n",
        "        #image = img_to_array(image)\n",
        "        train1.append (image)\n",
        "        y_train1.append(category_id)\n",
        "train = pd.DataFrame(train, columns=['file', 'category_id', 'category'])\n",
        "\n",
        "for file in glob.glob(os.path.join('{}/*.png'.format(validation_dir))):\n",
        "        test.append([file])\n",
        "        image = cv2.imread (file)\n",
        "        image = cv2.resize(remove_white(hsvmask(image)),(image_size,image_size))\n",
        "        #image = load_img(file, target_size=(200, 200))\n",
        "        #image = img_to_array(image)\n",
        "        validation.append (image)\n",
        "test = pd.DataFrame(test, columns=['file'])\n",
        "\n",
        "len(train1)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4750"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "5Ly6Mpg0X1pC",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_datagen = ImageDataGenerator(rotation_range=360,\n",
        "                                   zoom_range=0.1,\n",
        "                                   width_shift_range=0.1,\n",
        "                                   height_shift_range=0.1,\n",
        "                                   horizontal_flip=True,\n",
        "                                   vertical_flip=True)\n",
        "    \n",
        "\n",
        "#test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "\n",
        "#train_generator = train_datagen.flow_from_directory(train_dir,\n",
        "#                                                   target_size=(image_size, image_size),\n",
        "#                                                   batch_size=16, \n",
        "#                                                   class_mode='categorical',\n",
        "#                                                   color_mode='rgb',\n",
        "#                                                   shuffle=True,\n",
        "#                                                   seed=42)\n",
        "\n",
        "#validation_generator = test_datagen.flow_from_directory(validation_dir,\n",
        "#                                                       shuffle=False,\n",
        "#                                                       target_size=(image_size, image_size),\n",
        "#                                                       batch_size=16, \n",
        "#                                                       class_mode='categorical',\n",
        "#                                                       color_mode='rgb',\n",
        "#                                                       seed=42\n",
        "#                                                       )\n",
        "#plt.imshow(image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "S0T7aDQ3bWxa",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#files = []\n",
        "#files.append(os.listdir(f'{train_dir}/Black-grass')[:3])\n",
        "#files.append(os.listdir(f'{train_dir}/Charlock')[:3])\n",
        "#files.append(os.listdir(f'{train_dir}/Cleavers')[:3])\n",
        "#files.append(os.listdir(f'{train_dir}/Common Chickweed')[:3])\n",
        "#files.append(os.listdir(f'{train_dir}/Common wheat')[:3])\n",
        "#files.append(os.listdir(f'{train_dir}/Fat Hen')[:3])\n",
        "#files.append(os.listdir(f'{train_dir}/Loose Silky-bent')[:3])\n",
        "#files.append(os.listdir(f'{train_dir}/Maize')[:3])\n",
        "#files.append(os.listdir(f'{train_dir}/Scentless Mayweed')[:3])\n",
        "#files.append(os.listdir(f'{train_dir}/Shepherds Purse')[:3])\n",
        "#files.append(os.listdir(f'{train_dir}/Small-flowered Cranesbill')[:3])\n",
        "#files.append(os.listdir(f'{train_dir}/Sugar beet')[:3])\n",
        "\n",
        "#print(train_dir + '/' + files[0][0])\n",
        "#img = plt.imread(os.path.join(train_dir,CATEGORIES[0], files[0][0]))\n",
        "#plt.imshow(img)\n",
        "#count = 1\n",
        "#for i in range(len(files)):\n",
        "#  for j in range(3):\n",
        "#    f = plt.figure(figsize=(30,30))\n",
        "#    img = plt.imread(os.path.join(train_dir,CATEGORIES[i], files[i][j]))\n",
        "#    f.add_subplot(12,3,count)\n",
        "#    count = count + 1\n",
        "#    #plt.imshow(remove_white(hsvmask(img)), aspect=1)\n",
        "#    plt.imshow(img, aspect=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qhcEH3bLaeN6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#count = 1\n",
        "#for i in range(len(files)):\n",
        "#  for j in range(3):\n",
        "#    f = plt.figure(figsize=(30,30))\n",
        "#    img = plt.imread(os.path.join(train_dir,CATEGORIES[i], files[i][j]))\n",
        "#    f.add_subplot(12,3,count)\n",
        "#    count = count + 1\n",
        "#    plt.imshow(hsv(img), aspect=1)\n",
        "#    #plt.imshow(img, aspect=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yuhnizokN0VF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#count = 1\n",
        "#for i in range(len(files)):\n",
        "#  for j in range(3):\n",
        "#    f = plt.figure(figsize=(30,30))\n",
        "#    img = plt.imread(os.path.join(train_dir,CATEGORIES[i], files[i][j]))\n",
        "#    f.add_subplot(12,3,count)\n",
        "#    count = count + 1\n",
        "#    plt.imshow(remove_white(hsvmask(img)), aspect=1)\n",
        "#    #plt.imshow(img, aspect=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SXLoEtWG__bn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#files2 = []\n",
        "#files2.append(os.listdir(f'{validation_dir}')[:20])\n",
        "#\n",
        "#print(len(files2[0]))\n",
        "#\n",
        "#print(train_dir + '/' + files[0][0])\n",
        "#img = plt.imread(os.path.join(validation_dir, files2[0][0]))\n",
        "##plt.imshow(img)\n",
        "#count = 1\n",
        "#for i in range(len(files2)):\n",
        "#  for j in range(20):\n",
        "#    f = plt.figure(figsize=(30,30))\n",
        "#    img = plt.imread(os.path.join(validation_dir, files2[i][j]))\n",
        "#    f.add_subplot(12,3,count)\n",
        "#    count = count + 1\n",
        "#    #plt.imshow(img, aspect=1)\n",
        "#    plt.imshow(remove_white(hsvmask(img)), aspect=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "NFuQ5vdNfuZh",
        "outputId": "fa57baea-2721-4bc6-f012-2bb47ea7664d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "cell_type": "code",
      "source": [
        "#validation_generator\n",
        "#first_image = validation_generator.next()\n",
        "#first_image\n",
        "os.listdir(f'{train_dir}')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Black-grass',\n",
              " 'Charlock',\n",
              " 'Cleavers',\n",
              " 'Common Chickweed',\n",
              " 'Common wheat',\n",
              " 'Fat Hen',\n",
              " 'Loose Silky-bent',\n",
              " 'Maize',\n",
              " 'Scentless Mayweed',\n",
              " 'Shepherds Purse',\n",
              " 'Small-flowered Cranesbill',\n",
              " 'Sugar beet']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "DM6TYAr5DhPv",
        "colab_type": "code",
        "outputId": "1d3888e2-725c-486d-cb45-8a05f4cf693a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#green = np.uint8([[[0,30,30]]])\n",
        "#hsv_green = cv2.cvtColor(green,cv2.COLOR_BGR2HSV)\n",
        "#print(hsv_green)\n",
        "np.array(y_train1).shape"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4750,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "F0R40xk3f5-l",
        "outputId": "e49ca316-5dd4-47cf-82af-f9fc482104d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "cell_type": "code",
      "source": [
        "class_size = [len(files) for r, d, files in os.walk(train_dir)][1:]\n",
        "class_size = [max(class_size) / x for x in class_size]\n",
        "class_weights = {}\n",
        "for i in range(len(class_size)):\n",
        "  class_weights.update({i: class_size[i]})\n",
        "#[len(files) for r, d, files in os.walk(train_dir)][1:]\n",
        "class_weights[0] = class_weights[0]*1.5\n",
        "#class_weights[2] = class_weights[2]*3\n",
        "#class_weights[4] = class_weights[4]*3\n",
        "class_weights[6] = class_weights[6]*1.5\n",
        "#class_weights[8] = class_weights[8]*1.5\n",
        "class_weights"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 3.7300380228136882,\n",
              " 1: 1.676923076923077,\n",
              " 2: 2.278745644599303,\n",
              " 3: 1.0703764320785598,\n",
              " 4: 2.9592760180995477,\n",
              " 5: 1.3768421052631579,\n",
              " 6: 1.5,\n",
              " 7: 2.9592760180995477,\n",
              " 8: 1.2674418604651163,\n",
              " 9: 2.831168831168831,\n",
              " 10: 1.3185483870967742,\n",
              " 11: 1.6987012987012986}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "jitW6SUOL0JZ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#for i in np.random.randint(0, X_train.shape[0], 5):\n",
        "  #print('\\n', y_train[i])\n",
        "  #plt.imshow(X_train[i])\n",
        "  #plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "bOBZkJ4dL7wi",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "str(datetime.date.today())\n",
        "\n",
        "filepath = \"/content/drive/My Drive/Machine Learning/\" + str(datetime.date.today()) + \"-weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath,monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "tFBKNf4rwQ54",
        "outputId": "fc939c86-7a46-4848-921c-d3d9a2db54ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1028
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train,y_test = train_test_split(np.array(train1),np.array(y_train1), test_size=0.2, shuffle=True, random_state=42)\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train = X_train/255\n",
        "X_test = X_test/255\n",
        "#train_datagen.fit(X_train)\n",
        "def get_model():\n",
        "    model = Sequential()\n",
        "  \n",
        "    #model.add(Conv2D(filters=32, kernel_size=(3,3), padding='valid', input_shape=X_train[0].shape, activation='relu'))\n",
        "    #normalize\n",
        "  \n",
        "  \n",
        "    #1st conv\n",
        "    model.add(Conv2D(filters=32, kernel_size=(3,3), strides=2, padding='same',input_shape=X_train[0].shape))\n",
        "    #model.add(LeakyReLU(alpha=0.1))\n",
        "    model.add(BatchNormalization())\n",
        "    #model.add(LeakyReLU(alpha=0.1))\n",
        "    model.add(Conv2D(filters=32, kernel_size=(3,3), padding='valid'))\n",
        "    #model.add(BatchNormalization(axis=3))\n",
        "    #model.add(LeakyReLU(alpha=0.1))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    model.add(BatchNormalization())\n",
        "    #model.add(Dropout(0.1))\n",
        "  \n",
        "    #2nd conv\n",
        "    model.add(Conv2D(filters=64, kernel_size=(3,3), strides=2, padding='same'))\n",
        "    #model.add(LeakyReLU(alpha=0.1))\n",
        "    model.add(BatchNormalization())\n",
        "    #model.add(LeakyReLU(alpha=0.1))\n",
        "    model.add(Conv2D(filters=64, kernel_size=(3,3), padding='valid'))\n",
        "    #model.add(BatchNormalization(axis=3))\n",
        "    #model.add(LeakyReLU(alpha=0.1))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    model.add(BatchNormalization())\n",
        "    #model.add(Dropout(0.1))\n",
        "    \n",
        "    #3rd conv\n",
        "    model.add(Conv2D(filters=128, kernel_size=(3,3), strides=2, padding='same'))\n",
        "    #model.add(LeakyReLU(alpha=0.1))\n",
        "    model.add(BatchNormalization())\n",
        "    #model.add(LeakyReLU(alpha=0.1))\n",
        "    model.add(Conv2D(filters=128, kernel_size=(3,3), padding='valid'))\n",
        "    #model.add(BatchNormalization(axis=3))\n",
        "    #model.add(LeakyReLU(alpha=0.1))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    model.add(BatchNormalization())\n",
        "    #model.add(Dropout(0.1))\n",
        "    \n",
        "    #4th conv\n",
        "    #model.add(Conv2D(filters=256, kernel_size=(3,3), padding='valid', activation='relu'))\n",
        "    #model.add(LeakyReLU(alpha=0.1))\n",
        "    #model.add(BatchNormalization(axis=3))\n",
        "    #model.add(LeakyReLU(alpha=0.1))\n",
        "    #model.add(Conv2D(filters=256, kernel_size=(3,3), padding='valid', activation='relu'))\n",
        "    #model.add(BatchNormalization(axis=3))\n",
        "    #model.add(LeakyReLU(alpha=0.1))\n",
        "    #model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    #model.add(BatchNormalization(axis=3))\n",
        "    #model.add(Dropout(0.1))\n",
        "  \n",
        "    #flatten\n",
        "    model.add(GlobalAveragePooling2D())\n",
        "    #model.add(Flatten())\n",
        "  \n",
        "    #dense layer\n",
        "    model.add(Dense(units=128, activation='tanh'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Dense(units=128, activation='tanh'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.1))      \n",
        "    model.add(Dense(units=12, activation='softmax'))\n",
        "  \n",
        "    #dropout\n",
        "    #model.add(Dropout(0.3))\n",
        "  \n",
        "    return model\n",
        "  \n",
        "# test model\n",
        "epochs = 50\n",
        "learning_rate = 0.001\n",
        "decay_rate = learning_rate / epochs\n",
        "momentum = 0.9\n",
        "sgd = keras.optimizers.SGD(lr=learning_rate, momentum=momentum, nesterov=True)\n",
        "adam = keras.optimizers.Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=True)\n",
        "model_sgd = get_model()\n",
        "model_sgd.summary()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 112, 112, 32)      896       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 112, 112, 32)      128       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 110, 110, 32)      9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 55, 55, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 55, 55, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 28, 28, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 28, 28, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 26, 26, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 13, 13, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 7, 7, 128)         73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 7, 7, 128)         512       \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 5, 5, 128)         147584    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 2, 2, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 2, 2, 128)         512       \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_1 ( (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 12)                1548      \n",
            "=================================================================\n",
            "Total params: 324,396\n",
            "Trainable params: 322,988\n",
            "Non-trainable params: 1,408\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qEVn7hDHt0vb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras import metrics\n",
        "X_train, X_test, y_train,y_test = train_test_split(np.array(train1),np.array(y_train1), test_size=0.2, shuffle=True, random_state=42)\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train = X_train/255\n",
        "X_test = X_test/255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OQhBy8Dkg1bV",
        "colab_type": "code",
        "outputId": "2c587e43-4973-498b-d44f-f239a9324588",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 709
        }
      },
      "cell_type": "code",
      "source": [
        "model_sgd.compile(loss='sparse_categorical_crossentropy', optimizer=sgd, metrics=['acc'])\n",
        "#history_sgd = model_sgd.fit(X_train,y_train, batch_size=16, epochs=60, validation_data=(X_test,y_test), shuffle=True, callbacks=callbacks_list)\n",
        "#model_sgd.load_weights('/content/drive/My Drive/Machine Learning/2019-04-10-weights-improvement-02-0.52.hdf5')\n",
        "history_sgd = model_sgd.fit_generator(train_datagen.flow(X_train, y_train, batch_size=16),\n",
        "                                     validation_data=(X_test,y_test),\n",
        "                                     steps_per_epoch=X_train.shape[0]//16,\n",
        "                                     epochs=10,\n",
        "                                     callbacks=callbacks_list,\n",
        "                                     class_weight = class_weights,\n",
        "                                     verbose=1,\n",
        "                                     shuffle=True)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "237/237 [==============================] - 35s 147ms/step - loss: 0.4145 - acc: 0.9130 - val_loss: 0.2659 - val_acc: 0.9116\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.91158\n",
            "Epoch 2/10\n",
            "237/237 [==============================] - 35s 149ms/step - loss: 0.4402 - acc: 0.9159 - val_loss: 0.2793 - val_acc: 0.8947\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.91158\n",
            "Epoch 3/10\n",
            "237/237 [==============================] - 36s 150ms/step - loss: 0.4329 - acc: 0.9106 - val_loss: 0.2673 - val_acc: 0.9095\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.91158\n",
            "Epoch 4/10\n",
            "237/237 [==============================] - 35s 149ms/step - loss: 0.4369 - acc: 0.9095 - val_loss: 0.2632 - val_acc: 0.9095\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.91158\n",
            "Epoch 5/10\n",
            "237/237 [==============================] - 34s 144ms/step - loss: 0.4243 - acc: 0.9153 - val_loss: 0.2746 - val_acc: 0.9042\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.91158\n",
            "Epoch 6/10\n",
            "237/237 [==============================] - 35s 147ms/step - loss: 0.4226 - acc: 0.9151 - val_loss: 0.2716 - val_acc: 0.9084\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.91158\n",
            "Epoch 7/10\n",
            "237/237 [==============================] - 35s 149ms/step - loss: 0.4388 - acc: 0.9153 - val_loss: 0.2764 - val_acc: 0.9032\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.91158\n",
            "Epoch 8/10\n",
            "237/237 [==============================] - 34s 143ms/step - loss: 0.4645 - acc: 0.9088 - val_loss: 0.2778 - val_acc: 0.9074\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.91158\n",
            "Epoch 9/10\n",
            "237/237 [==============================] - 36s 150ms/step - loss: 0.4345 - acc: 0.9106 - val_loss: 0.2578 - val_acc: 0.9095\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.91158\n",
            "Epoch 10/10\n",
            "237/237 [==============================] - 34s 144ms/step - loss: 0.4278 - acc: 0.9151 - val_loss: 0.2749 - val_acc: 0.9074\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.91158\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ez1ZoTj4Jmp7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_sgd.load_weights('/content/drive/My Drive/Machine Learning/absolutebest.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M3D3Z-g0kaMb",
        "colab_type": "code",
        "outputId": "32041747-b80c-4e6b-bf7c-c7b5aedae052",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "# PREDICTIONS\n",
        "y_pred = model_sgd.predict(X_test)\n",
        "#print(y_pred)\n",
        "y_class = np.argmax(y_pred, axis=1)\n",
        "#print(y_class)\n",
        "#print(y_test)\n",
        "y_check = y_test\n",
        "#print(y_check)\n",
        "cmatrix = confusion_matrix(y_check, y_class)\n",
        "print(cmatrix)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 58   0   0   0   2   0   7   0   0   0   0   0]\n",
            " [  0  79   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   1  54   1   0   0   0   0   0   0   1   0]\n",
            " [  1   0   2 114   0   0   2   1   0   1   0   4]\n",
            " [  1   0   0   0  34   0   0   0   0   0   0   0]\n",
            " [  0   0   1   1   4  73   0   0   0   0   0   1]\n",
            " [ 36   0   0   0   2   0 101   0   1   0   0   0]\n",
            " [  0   0   0   0   0   0   0  45   0   0   0   1]\n",
            " [  2   0   0   3   0   0   0   0  87   1   0   1]\n",
            " [  0   1   0   3   0   0   0   0   1  37   0   0]\n",
            " [  0   1   0   0   0   0   0   1   0   0  99   0]\n",
            " [  0   1   0   0   0   0   0   2   0   0   0  81]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LHzAt9Z9BxNA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "retrain_dir1 = '/content/drive/My Drive/Machine Learning/plant-seedlings-classification/train/' + CATEGORIES[0]\n",
        "retrain_dir2 = '/content/drive/My Drive/Machine Learning/plant-seedlings-classification/train/' + CATEGORIES[6]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VDS-1O7JGfyY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "retrain = []\n",
        "retrain1 = []\n",
        "y_retrain = []\n",
        "\n",
        "for file in glob.glob(os.path.join('{}/*.png'.format(retrain_dir1))):\n",
        "    retrain.append([file, '0', CATEGORIES[0]])\n",
        "    image = cv2.imread (file)\n",
        "    image = cv2.resize(remove_white(hsvmask(image)),(200,200))\n",
        "    #image = load_img(file, target_size=(200, 200))\n",
        "    #image = img_to_array(image)\n",
        "    retrain1.append(image)\n",
        "    y_retrain.append('0')\n",
        "for file in glob.glob(os.path.join('{}/*.png'.format(retrain_dir2))):\n",
        "    retrain.append([file, '6', CATEGORIES[6]])\n",
        "    image = cv2.imread (file)\n",
        "    image = cv2.resize(remove_white(hsvmask(image)),(200,200))\n",
        "    #image = load_img(file, target_size=(200, 200))\n",
        "    #image = img_to_array(image)\n",
        "    retrain1.append(image)\n",
        "    y_retrain.append('6')\n",
        "retrain = pd.DataFrame(retrain, columns=['file', 'category_id', 'category'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z_S_xA3rDU9T",
        "colab_type": "code",
        "outputId": "5eec95c0-bc09-4f2e-a57c-ca44ee288885",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "[len(files) for r, d, files in os.walk(train_dir)]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 263, 390, 287, 611, 221, 475, 654, 221, 516, 231, 496, 385]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "metadata": {
        "id": "NDU6wb2R1fPG",
        "colab_type": "code",
        "outputId": "416e5bda-4f40-4e69-f223-6ee8f7eac058",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        }
      },
      "cell_type": "code",
      "source": [
        "for layer in model_sgd.layers:\n",
        "  layer.trainable = True\n",
        "for layer in model_sgd.layers[:-7]:\n",
        "  layer.trainable = False\n",
        "\n",
        "for layer in model_sgd.layers:\n",
        "  print(layer, layer.trainable)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<keras.layers.convolutional.Conv2D object at 0x7f2d0f0d7eb8> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7f2d0f0e6358> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f2d0f0e62e8> False\n",
            "<keras.layers.pooling.MaxPooling2D object at 0x7f2d0f0e6400> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7f2d005b8f98> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f2d005b8978> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7f2d0045c8d0> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f2d004bde80> False\n",
            "<keras.layers.pooling.MaxPooling2D object at 0x7f2d00368a20> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7f2d003c8b00> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f2d002fbd68> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7f2d002ac8d0> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f2d0026ad68> False\n",
            "<keras.layers.pooling.MaxPooling2D object at 0x7f2d001b6828> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7f2d00171550> False\n",
            "<keras.layers.pooling.GlobalAveragePooling2D object at 0x7f2d00171588> False\n",
            "<keras.layers.core.Dense object at 0x7f2d000c15c0> True\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7f2ce1ffc6d8> True\n",
            "<keras.layers.core.Dropout object at 0x7f2ce1ffc3c8> True\n",
            "<keras.layers.core.Dense object at 0x7f2ce1faea58> True\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7f2ce1f65f98> True\n",
            "<keras.layers.core.Dropout object at 0x7f2ce1f089b0> True\n",
            "<keras.layers.core.Dense object at 0x7f2ce1e00a90> True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OOzP8LQ4FplC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train2, X_test2, y_train2,y_test2 = train_test_split(np.array(retrain1),np.array(y_retrain), test_size=0.2, random_state=42)\n",
        "X_train2 = X_train2.astype('float32')\n",
        "X_test2 = X_test2.astype('float32')\n",
        "X_train2 = X_train2/255\n",
        "X_test2 = X_test2/255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cTDQjg8xG-Pc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_sgd.compile(loss='sparse_categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qX0LHTR_F_Ec",
        "colab_type": "code",
        "outputId": "22c4fc13-05d0-4775-e283-0456b07303cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "cell_type": "code",
      "source": [
        "history_sgd = model_sgd.fit_generator(train_datagen.flow(X_train2, y_train2, batch_size=16),\n",
        "                                     validation_data=(X_test2,y_test2),\n",
        "                                     epochs=1,\n",
        "                                     steps_per_epoch=X_train2.shape[0]//16,\n",
        "                                     callbacks=callbacks_list,\n",
        "                                     verbose=1,\n",
        "                                     shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "45/45 [==============================] - 8s 181ms/step - loss: 3.9773 - acc: 0.2250 - val_loss: 1.0680 - val_acc: 0.7228\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86421\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Lx3itkNgFgaa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from google.colab import files\n",
        "\n",
        "savePath = '/content/drive/My Drive/Machine Learning/'\n",
        "\n",
        "#Y_new = model_sgd.predict_classes(validation)\n",
        "with open(str(savePath)+'submission.csv','w') as f1:\n",
        "  writer = csv.writer(f1, delimiter=',', lineterminator='\\n')\n",
        "  #print(validation[0])\n",
        "  #print(Y_new[0])\n",
        "  writer.writerow(['file','species'])\n",
        "  for i in range(len(validation)):\n",
        "    xid = test.iloc[i]['file'].replace(validation_dir, '')[1:]\n",
        "    data = CATEGORIES[model_sgd.predict_classes(np.expand_dims(validation[i]/255, axis=0))[0]]\n",
        "    writer.writerow([xid, data])\n",
        "  #files.download(str(savePath)+'submission.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "rLh01_NYPtRv"
      },
      "cell_type": "markdown",
      "source": [
        "## The Learning Rate"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "H6iPjqZOPtRx"
      },
      "cell_type": "markdown",
      "source": [
        "The learning rate is an hyper-parameter that controls how much we are  adjusting the weights of our network with respect to the loss gradient. \n",
        "\n",
        "The lower this value the slower the travel along the downward slope, if the value is higher the contrary will happen. Even though having a low value might sounds like a good idea, so we make sure we don't miss the minima loss value it would also mean that we can get stuck in a local minima easier and we'll take a longer time to converge.\n",
        "\n",
        "The following formula shows how the learning rate affects the changes in the weights during the learning process:"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "2b7c0qI2PtRx"
      },
      "cell_type": "markdown",
      "source": [
        "``new_weight = existing_weight — learning_rate * gradient``"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "37XgDWSDPtRy"
      },
      "cell_type": "markdown",
      "source": [
        "### Stochastic gradient descent(SGD)"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "kkdamS0bPtRz"
      },
      "cell_type": "markdown",
      "source": [
        "In keras SGD learning comes with Nesterov enabled by defult. Nesterov accelerated grading is a different version of the momentum method which has stronger theoretical converge guarantees for convex functions. \n",
        "\n",
        "![Nesterov](Notebook/Nesterov.png)\n",
        "\n",
        "**Good for:**\n",
        "* Shallow networks"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "hTNU9TP8PtR0",
        "outputId": "f2655f9d-ff14-4538-d1d8-b87595c111cb",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "keras.optimizers.SGD(lr=10**-3, nesterov=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.optimizers.SGD at 0x7fb58c1fcc88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Fg7n8ZaXPtR5"
      },
      "cell_type": "markdown",
      "source": [
        "#### Time-based decay\n",
        "\n",
        "The SGD optimizer takes decay and lr arguments and updates the learning rate by a decreasing factor in each epoch.\n",
        "\n",
        "#### Momentum\n",
        "\n",
        "Momentum is another argument in SGD optimizer which we could tweak to obtain faster convergence. Unlike classical SGD, momentum method helps the parameter vector to build up velocity in any direction with constant gradient descent so as to prevent oscillations. A typical choice of momentum is between 0.5 to 0.9."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "NxJXvpcDPtR6",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "decay_rate = learning_rate / epochs\n",
        "momentum = 0.9\n",
        "sgd = keras.optimizers.SGD(lr=learning_rate, momentum=momentum, nesterov=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "K5HYeoMPrd7j",
        "outputId": "5704982a-7b2a-4e13-aefd-08437da102a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 840
        }
      },
      "cell_type": "code",
      "source": [
        "model_sgd = get_model()\n",
        "model_sgd.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_19 (Conv2D)           (None, 198, 198, 32)      896       \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 196, 196, 32)      9248      \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 196, 196, 32)      128       \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 196, 196, 32)      0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling (None, 98, 98, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_21 (Conv2D)           (None, 96, 96, 32)        9248      \n",
            "_________________________________________________________________\n",
            "conv2d_22 (Conv2D)           (None, 94, 94, 32)        9248      \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 94, 94, 32)        128       \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 94, 94, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling (None, 47, 47, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_23 (Conv2D)           (None, 45, 45, 64)        18496     \n",
            "_________________________________________________________________\n",
            "conv2d_24 (Conv2D)           (None, 43, 43, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 43, 43, 64)        256       \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 43, 43, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_12 (MaxPooling (None, 21, 21, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 28224)             0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 512)               14451200  \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 128)               65664     \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 30)                1950      \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 12)                372       \n",
            "=================================================================\n",
            "Total params: 14,612,018\n",
            "Trainable params: 14,611,762\n",
            "Non-trainable params: 256\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "nPahRz_-tXsS",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_sgd.compile(loss='sparse_categorical_crossentropy', optimizer=keras.optimizers.SGD(), metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "nVQ7xbQItzsC",
        "outputId": "ee93ec11-fe97-4479-af51-0841f1a69c58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1042
        }
      },
      "cell_type": "code",
      "source": [
        "history_sgd = model_sgd.fit(X_train,y_train, batch_size=16, epochs=60, validation_data=(X_test,y_test), shuffle=True)\n",
        "#np.array(train1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 2138 samples, validate on 535 samples\n",
            "Epoch 1/60\n",
            "2138/2138 [==============================] - 19s 9ms/step - loss: 1.3630 - acc: 0.5042 - val_loss: 3.2443 - val_acc: 0.3551\n",
            "Epoch 2/60\n",
            "2138/2138 [==============================] - 18s 8ms/step - loss: 0.7050 - acc: 0.7404 - val_loss: 1.1600 - val_acc: 0.6187\n",
            "Epoch 3/60\n",
            "2138/2138 [==============================] - 18s 8ms/step - loss: 0.4437 - acc: 0.8358 - val_loss: 9.2388 - val_acc: 0.1682\n",
            "Epoch 4/60\n",
            "2138/2138 [==============================] - 17s 8ms/step - loss: 0.2426 - acc: 0.9167 - val_loss: 6.0546 - val_acc: 0.2486\n",
            "Epoch 5/60\n",
            "2138/2138 [==============================] - 17s 8ms/step - loss: 0.1406 - acc: 0.9509 - val_loss: 0.8563 - val_acc: 0.7140\n",
            "Epoch 6/60\n",
            "2138/2138 [==============================] - 17s 8ms/step - loss: 0.0945 - acc: 0.9724 - val_loss: 1.2947 - val_acc: 0.6374\n",
            "Epoch 7/60\n",
            "2138/2138 [==============================] - 18s 8ms/step - loss: 0.1232 - acc: 0.9654 - val_loss: 2.0899 - val_acc: 0.4542\n",
            "Epoch 8/60\n",
            "2138/2138 [==============================] - 18s 8ms/step - loss: 0.0580 - acc: 0.9841 - val_loss: 3.8284 - val_acc: 0.4037\n",
            "Epoch 9/60\n",
            "2138/2138 [==============================] - 18s 8ms/step - loss: 0.0401 - acc: 0.9892 - val_loss: 2.4731 - val_acc: 0.4935\n",
            "Epoch 10/60\n",
            "2138/2138 [==============================] - 18s 8ms/step - loss: 0.0398 - acc: 0.9906 - val_loss: 2.9736 - val_acc: 0.4673\n",
            "Epoch 11/60\n",
            "2138/2138 [==============================] - 17s 8ms/step - loss: 0.0320 - acc: 0.9925 - val_loss: 0.8420 - val_acc: 0.7850\n",
            "Epoch 12/60\n",
            "2138/2138 [==============================] - 17s 8ms/step - loss: 0.0078 - acc: 0.9986 - val_loss: 0.8166 - val_acc: 0.7925\n",
            "Epoch 13/60\n",
            "2138/2138 [==============================] - 17s 8ms/step - loss: 0.0051 - acc: 0.9986 - val_loss: 0.7841 - val_acc: 0.8168\n",
            "Epoch 14/60\n",
            "2138/2138 [==============================] - 17s 8ms/step - loss: 0.0222 - acc: 0.9939 - val_loss: 1.1051 - val_acc: 0.7458\n",
            "Epoch 15/60\n",
            "2138/2138 [==============================] - 18s 8ms/step - loss: 0.0065 - acc: 0.9991 - val_loss: 1.7086 - val_acc: 0.6467\n",
            "Epoch 16/60\n",
            "2138/2138 [==============================] - 18s 8ms/step - loss: 0.0050 - acc: 0.9981 - val_loss: 0.7693 - val_acc: 0.8093\n",
            "Epoch 17/60\n",
            "2138/2138 [==============================] - 18s 8ms/step - loss: 0.0026 - acc: 1.0000 - val_loss: 0.7871 - val_acc: 0.8019\n",
            "Epoch 18/60\n",
            "2138/2138 [==============================] - 17s 8ms/step - loss: 0.0054 - acc: 0.9986 - val_loss: 0.7924 - val_acc: 0.8037\n",
            "Epoch 19/60\n",
            "2138/2138 [==============================] - 18s 8ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.7879 - val_acc: 0.8150\n",
            "Epoch 20/60\n",
            "2138/2138 [==============================] - 18s 8ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.7764 - val_acc: 0.8168\n",
            "Epoch 21/60\n",
            "2138/2138 [==============================] - 18s 8ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.8178 - val_acc: 0.8112\n",
            "Epoch 22/60\n",
            "2138/2138 [==============================] - 17s 8ms/step - loss: 9.3533e-04 - acc: 1.0000 - val_loss: 0.8157 - val_acc: 0.8131\n",
            "Epoch 23/60\n",
            "2138/2138 [==============================] - 17s 8ms/step - loss: 4.9874e-04 - acc: 1.0000 - val_loss: 0.8890 - val_acc: 0.8093\n",
            "Epoch 24/60\n",
            "2138/2138 [==============================] - 18s 8ms/step - loss: 5.8542e-04 - acc: 1.0000 - val_loss: 0.8016 - val_acc: 0.8150\n",
            "Epoch 25/60\n",
            "2138/2138 [==============================] - 17s 8ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.8939 - val_acc: 0.8168\n",
            "Epoch 26/60\n",
            "2138/2138 [==============================] - 18s 8ms/step - loss: 6.4026e-04 - acc: 1.0000 - val_loss: 0.8580 - val_acc: 0.8056\n",
            "Epoch 27/60\n",
            "2138/2138 [==============================] - 17s 8ms/step - loss: 5.0670e-04 - acc: 1.0000 - val_loss: 0.7631 - val_acc: 0.8243\n",
            "Epoch 28/60\n",
            "2138/2138 [==============================] - 18s 8ms/step - loss: 6.2632e-04 - acc: 1.0000 - val_loss: 0.8083 - val_acc: 0.8150\n",
            "Epoch 29/60\n",
            "2138/2138 [==============================] - 18s 8ms/step - loss: 9.9737e-04 - acc: 1.0000 - val_loss: 0.7725 - val_acc: 0.8262\n",
            "Epoch 30/60\n",
            "1248/2138 [================>.............] - ETA: 6s - loss: 0.0019 - acc: 0.9992"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "DRGpgkU8wQcN",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Plot training & validation accuracy values\n",
        "plt.plot(history_sgd.history['acc'])\n",
        "plt.plot(history_sgd.history['val_acc'])\n",
        "plt.title('SGD accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history_sgd.history['loss'])\n",
        "plt.plot(history_sgd.history['val_loss'])\n",
        "plt.title('SGD loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "hK5ZR-1HPtR9"
      },
      "cell_type": "markdown",
      "source": [
        "## RMSprop"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "QTjRt2SaPtR9"
      },
      "cell_type": "markdown",
      "source": [
        "It is an unpublished, adaptive learning rate method proposed by Geoff Hinton in Lecture 6e of his Coursera Class.\n",
        "\n",
        "Rather than having just a global, scalar learning rate, we have a vector of learning rates for each trainable parameter. It is iteratively updated with a running average of magnitudes of squares of previous gradients. Changes to the weights during training are now not purely in the direction of the gradient, but rather in the direction of the elementwise division of the gradient by this vector you are maintaining. "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "LtTUMyROPtR_",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "teINO9SR045E",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_rmsprop = get_model()\n",
        "model_rmsprop.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Fmug3QlW1GDy",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_rmsprop.compile(loss='sparse_categorical_crossentropy', optimizer=keras.optimizers.RMSprop(), metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "mivL6f9-1JpW",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "history_rmsprop = model_rmsprop.fit(X_train, y_train, batch_size=16, epochs=60, validation_data=(X_test, y_test), shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "SMeCtydY1Soq",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Plot training & validation accuracy values\n",
        "plt.plot(history_rmsprop.history['acc'])\n",
        "plt.plot(history_rmsprop.history['val_acc'])\n",
        "plt.title('RMSprop accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history_rmsprop.history['loss'])\n",
        "plt.plot(history_rmsprop.history['val_loss'])\n",
        "plt.title('RMSprop loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "GMMj4BxJPtSB"
      },
      "cell_type": "markdown",
      "source": [
        "## Adam"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "XLuPY5kHPtSC"
      },
      "cell_type": "markdown",
      "source": [
        "Adaptive moment estimation\n",
        "\n",
        "Adam = RMSprop + Momentum\n",
        "\n",
        "**Good for:**\n",
        "\n",
        "* Relatively low memory requirements (though higher than gradient descent and gradient descent with momentum)  \n",
        "* Usually works well even with little tuning of hyperparameters.\n",
        "\n",
        "In Keras, we can define it like this:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Ona8FelSPtSE",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "keras.optimizers.Adam(lr=0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "s06uK-BL3BaC",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_adam = get_model()\n",
        "model_adam.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "WbQHm9wI3ErV",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_adam.compile(loss='sparse_categorical_crossentropy', optimizer=keras.optimizers.Adam(), metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ilMZSFKg3Lib",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "history_adam = model_adam.fit(X_train, y_train, batch_size=16, epochs=60, validation_data=(X_test, y_test), shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "IJmRSzDP3TW5",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Plot training & validation accuracy values\n",
        "plt.plot(history_adam.history['acc'])\n",
        "plt.plot(history_adam.history['val_acc'])\n",
        "plt.title('Adam accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history_adam.history['loss'])\n",
        "plt.plot(history_adam.history['val_loss'])\n",
        "plt.title('Adam loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "D9Bc8gZ7PtSN"
      },
      "cell_type": "markdown",
      "source": [
        "## Adagrad"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "8Y4JI4_4PtSQ"
      },
      "cell_type": "markdown",
      "source": [
        "It makes big updates for infrequent parameters and small updates for frequent parameters. For this reason, it is well-suited for dealing with sparse data.\n",
        "\n",
        "The main benefit of Adagrad is that we don’t need to tune the learning rate manually. Most implementations use a default value of 0.01 and leave it at that.\n",
        "\n",
        "**Disadvantage:**\n",
        "\n",
        "Its main weakness is that its learning rate is always Decreasing and decaying."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "v86Zg3eCPtSR",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "keras.optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "8e0WKLEawtAD",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_adagrad = get_model()\n",
        "model_adagrad.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "mKYWAUIuxJtl",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_adagrad.compile(loss='sparse_categorical_crossentropy', optimizer=keras.optimizers.Adagrad(), metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "X72DU4KbxldT",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "history_adagrad = model_adagrad.fit(X_train, y_train, batch_size=128, epochs=60, validation_data=(X_test, y_test), shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "5AvETOddyxRB",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Plot training & validation accuracy values\n",
        "plt.plot(history_adagrad.history['acc'])\n",
        "plt.plot(history_adagrad.history['val_acc'])\n",
        "plt.title('Adagrad accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history_adagrad.history['loss'])\n",
        "plt.plot(history_adagrad.history['val_loss'])\n",
        "plt.title('Adagrad loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "xAx-ZMqkPtSZ"
      },
      "cell_type": "markdown",
      "source": [
        "## AdaDelta"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "MCefk5d6PtSd"
      },
      "cell_type": "markdown",
      "source": [
        "More robust extension of Adagrad that adapts learning rates based on a moving window of gradient updates, instead of accumulating all past gradients. This way, Adadelta continues learning even when many updates have been done. Compared to Adagrad, in the original version of Adadelta you don't have to set an initial learning rate. In this version, initial learning rate and decay factor can be set, as in most other Keras optimizers.\n",
        "\n",
        "It is recommended to leave the parameters of this optimizer at their default values."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "iZj_jVzDPtSk",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "yYGH2xirzEBw",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_adadelta = get_model()\n",
        "model_adadelta.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "uxLWaUr0zJio",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_adadelta.compile(loss='sparse_categorical_crossentropy', optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "FvlfPm4qzPwi",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "history_adadelta = model_adadelta.fit(X_train, y_train, batch_size=128, epochs=60, validation_data=(X_test, y_test), shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "9bXr7a8B2rPK",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Plot training & validation accuracy values\n",
        "plt.plot(history_adadelta.history['acc'])\n",
        "plt.plot(history_adadelta.history['val_acc'])\n",
        "plt.title('Adadelta accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history_adadelta.history['loss'])\n",
        "plt.plot(history_adadelta.history['val_loss'])\n",
        "plt.title('Adadelta loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "5yos7lxfPtSq"
      },
      "cell_type": "markdown",
      "source": [
        "<img src=\"Notebook/optimizer.gif\" alt=\"optimizers\">"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "i2ErvhpF49FH"
      },
      "cell_type": "markdown",
      "source": [
        "# General Plot"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Vbnp29ax4-yU",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.plot(history_sgd.history['acc'])\n",
        "plt.plot(history_adam.history['acc'])\n",
        "plt.plot(history_rmsprop.history['acc'])\n",
        "plt.plot(history_adadelta.history['acc'])\n",
        "plt.plot(history_adagrad.history['acc'])\n",
        "\n",
        "plt.title('Accuracies')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['SGD', 'Adam', 'RMSprop', 'Adadelta', 'Adagrad'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "VBIBfVOQ-BSa",
        "scrolled": true,
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "savePath = '/content/drive/My Drive/Machine Learning/'\n",
        "\n",
        "#Y_new = model_sgd.predict_classes(validation)\n",
        "with open(str(savePath)+'submission.csv','w') as f1:\n",
        "  writer = csv.writer(f1, delimiter=',', lineterminator='\\n')\n",
        "  #print(validation[0])\n",
        "  #print(Y_new[0])\n",
        "  for i in range(len(validation)):\n",
        "    xid = glob.glob(os.path.join('{}/*.png'.format(validation_dir)))[i].replace(validation_dir, '')[1:]\n",
        "    data = CATEGORIES[model_sgd.predict_classes(np.expand_dims(validation[i], axis=0))[0]]\n",
        "    writer.writerow([xid, data])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sqcZgJE-DhSR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}